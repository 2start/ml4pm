{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5.1: Logistic regression and Gaussian Processes\n",
    "\n",
    "Welcome to the fifth tutorial of the course 'Machine learning for Precision Medicine'.\n",
    "\n",
    "In this exercise, we want to predict benign and malignant breast cancer from biopsy images with a logistic regression model. We herefore use the WDBC dataset containing \n",
    "- 569 samples from patients with known diagnosis\n",
    "- 357 benign\n",
    "- 212 malignant\n",
    "- 30 features extracted from fine needle aspirate slides\n",
    "\n",
    "We are given a number of features that describe the cell nuclei that have been determined from image processing techniques [Street et al, 1992]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we have outsourced some functions, which we frequently use in a second python script 'util.py'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data with the load_data() function in util.py\n",
    "X_train, y_train = util.load_data('data.csv')\n",
    "X_test, y_test = util.load_data('data.csv', testing_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0:20] # 1 where malignant, 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification\n",
    "\n",
    "**Classification** refers to the task of predicting a **class label** $y$, *i.e.*, the diagnosis, from a **feature vector** $\\bf{x}$.\n",
    "For the case, where $y$ can take one of two values, we speak of binary classification.\n",
    "\n",
    "\n",
    "For the task at hand, this means that for the image features, we are\n",
    "then given a new image for which we don't know the diagnosis. We can predict the diagnosis based on what we have learned from from the training data.\n",
    "\n",
    "Similar to linear regression, we are trying to find an optimal weight vector $\\mathbf{w}$ that will minize our objective function. However, we have to make adjustments because we are no longer dealing with continous output variables, but binary class labels. In most real-world datasets, the two classes will not be perfectly separable, i.e. we expect to make mistakes and want to minimize those. Therefore we predict probabilities of belonging to class 1, instead of binary class labels. We can achieve this by making use of the logistic sigmoid link function. \n",
    "\n",
    "The **logistic** $\\pi(a)$ is a function between 0 and 1, making it suited for modeling probabilities. It is called a **sigmoid** function because of its *s*-shape.\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi(a) := \\frac{1}{1+\\exp{\\left(-a\\right)}} = \\frac{\\exp{\\left(a\\right)}}{1+\\exp{\\left(a\\right)}}\n",
    "\\end{equation}\n",
    "\n",
    "Here, as we are modeling linear functions, $a=\\mathbf{x}_n\\mathbf{w}$, where $\\mathbf{x}_n$ is the **feature vector** for the $n$-th individual (given), and $\\mathbf{w}$ is a **weight vector** that we would like to find.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: ##  \n",
    "Implement the logistic sigmoid following the formula above. We have to make sure that the output of this function is never exactly 0 or 1, because we will take the log of it. Therefore, we use np.clip() to limit range (from ... to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(a):\n",
    "    logist = # your_code\n",
    "    logist = # your_code\n",
    "    return logist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in every machine learning problem, we need to define an **objective** function $L$. It quantifies the error between our predictions and the ground-truth. We then determine the weights $\\mathbf{w}^{opt}$ that minimize $L$.\n",
    "\n",
    "We would like to assign high probability to all the instances that belong to the target class and low probability otherwise. Accordingly, we would like to obtain a function that records a loss, whenever we assign low probability to the correct class $c_{true}$.\n",
    "\n",
    "One function that achieves this is the **log-loss** or **cross-entropy** loss, which is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "loss = -\\sum_{n\\in c_1} \\ln( \\pi(\\mathbf{x}_n\\mathbf{w}) ) - \\sum_{n'\\in c_2} \\ln( 1-\\pi(\\mathbf{x}_{n'}\\mathbf{w}) )\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $c_1$ are all the members of the first class (here: malignant, `y == 1.`) and $c_2$ are all the members of the second class (benign, `y == 0.`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "Implement the log-loss (binary cross entropy function) using the formula above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y, y_hat):\n",
    "    \"\"\"\n",
    "    return the loss for predicted probabilities y_hat, and class labels y\n",
    "    Keyworld arguments\n",
    "    y -- scalar or numpy array\n",
    "    y_hat -- scalar or numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    loss = #your_code\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss(np.array([0.,1.,1.]), np.array([0.1, 0.5, 0.99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output **:\n",
    "0.80855803207127308"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken together, we obtain the **Logistic Regression objective** $L(\\mathbf{w})$.\n",
    "\\begin{equation}\n",
    "L(\\mathbf{w}) = \\underbrace{-\\sum_{n\\in c_1} \\ln( \\pi(\\mathbf{x}_n\\mathbf{w}) ) - \\sum_{n'\\in c_2} \\ln( 1-\\pi(\\mathbf{x}_{n'}\\mathbf{w}) )}_{loss} + \\underbrace{ \\lambda \\cdot 0.5 \\cdot \\sum_{d=1}^{D}{w_d}^2}_{regularizer}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3:  \n",
    "Implement the regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizer(w, lambd):\n",
    "    '''\n",
    "    return the value for the regularizer for a given w and lamd\n",
    "    ''' \n",
    "    reg = # your_code\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The derivative\n",
    "\n",
    "In order to minimize our objective function, we will make use of the derivative. The derivative will tell us in which direction we have to adjust our weights, in order to minimize the loss. Note that no analytical solition exists. Instead, we will have to optimize our objective using an optimization algorithm (see below). The derivative of the objective with respect to a single $w_d$ is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial w_d} = \\sum_n^{N}{x_{nd}} \\cdot\n",
    " \\left( \\pi\\left(\\mathbf{x}_n\\mathbf{w}_n\\right)-I\\left(\\mathbf{y}_n== c_1\\right)\\right) + \\lambda \\cdot w_d\n",
    "\\end{equation}\n",
    "\n",
    "With $I$ being the Identity matrix. $I(a==b)$ denotes the indicator function, which yields 1 if $a=b$ and 0 otherwise.\n",
    "\n",
    "The sign of the derivative indicates the direction in which the objective gets larger or smaller and the magnitude the rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient\n",
    "\n",
    "By stacking all partial derivatives into a single vector, we obtain the gradient $\\nabla_\\mathbf{w} (L)$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla{L}\\left(\\mathbf{w}^{t}\\right) =\n",
    "\\left[\\begin{matrix}\n",
    "\\frac{\\partial L}{\\partial w^t_1}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial L}{\\partial w^t_D}\n",
    "\\end{matrix}\\right]\n",
    "=\n",
    "\\underbrace{\\mathbf{X}^{T}\n",
    " \\left( \\pi\\left(\\mathbf{X}\\mathbf{w}^t\\right)-I\\left(\\mathbf{y}==c_1\\right)\\right)}_{\\nabla{\\text{loss}}\\left(\\mathbf{w}^{t}\\right)}+ \\underbrace{\\lambda \\cdot \\mathbf{w}^t}_{\\nabla{\\text{regularizer}}\\left(\\mathbf{w}^{t}\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "$\\nabla_\\mathbf{w} (L)$ is a $D$-dimensional vector pointing in the direction of steepest growth of the objective and in the opposite direction in which the steepest reduction.\n",
    "Using the gradient, we can define a simple optimization algorithm.\n",
    "\n",
    "#### Steepest descent\n",
    "\n",
    "The steepest descent algorithm uses the gradient by making small steps in the direction $-\\nabla_{\\mathbf{w}^{t}} (L)$. You can think about it as being on a hill and descending the hill in the steepest direction downwards.\n",
    "Therefore the algorithm is called **steepest descent**.\n",
    "\n",
    "given learning rate $0<\\alpha<1.0$ and current weight estimate $\\mathbf{w}^{t}$.\n",
    "Iterate by setting $\\mathbf{w}^{t+1} = \\mathbf{w}^{t} - \\alpha \\cdot \\nabla_{\\mathbf{w}^{t}} (L)$.\n",
    "\n",
    "A typical value for $\\alpha$ is around $10^{-4}$.\n",
    "\n",
    "A problem with steepest descent is that the estimate tends to oscillate and often even overshoots and diverges (leading to an increase in the objective). Getting the learning rate right is very hard, trading off progress in learning and risk of diverging. Many tricks exit to improve learning in gradient descent, such as weight decay, where the learning rate is gradually reduced during learning.\n",
    "\n",
    "Here we will implement the steepest_descent_optimizer as a class, which has the attributes: alpha, lamd, X, y, w and max_iter (maximum number of iterations) and the functions: predict(), optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "\n",
    "We will now implement the algorithm described above using a class Steepest_descent_optimizer(). \n",
    "\n",
    "You are given the template below, and are expected to complete the methods `_gradient()`, `_update()` and `optimize()`.\n",
    "\n",
    "Remember within the class you can access the current weights, X, y, value for the regularizer (`lambd`), etc using `self.w`, `self.X`, `self.y` and so on.\n",
    "\n",
    "The `_gradient()` method should use `self.X`, `self.y`, `self.w`, `self.lambd` ($\\lambda$) the `logistic()` function you implemented above, and return the gradient of the loss function with respect to `self.w` ($\\mathbf{w}$) (see formula above for $\\nabla{L}\\left(\\mathbf{w}^{t}\\right)$). Tip: for $(y == c_1)$ you can just use `self.y` directly.\n",
    "\n",
    "The `_update()` method should get the gradient using `self._gradient()` and update the current weights `self.w` using the learning rate (`self.alpha`) and the update rule for steepest descent described above: $\\mathbf{w}^{t+1} = \\mathbf{w}^{t} - \\alpha \\cdot \\nabla_{\\mathbf{w}^{t}} (L)$\n",
    "\n",
    "Finally, the `optimize()` function will update the weights for `self.max_iter` times, and after every update calculate the loss (including the regularizer) and store it in a list `loss`. Finally, it returns this `loss`-\"history\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Steepest_descent_optimizer():\n",
    "    \n",
    "    def __init__(self,X,y,lambd,alpha):\n",
    "        self.alpha = alpha\n",
    "        self.lambd = lambd\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.w = np.zeros(X.shape[1]) # we initialize the weights with zeros\n",
    "        \n",
    "        self.max_iter = 10000 # set the max number of iterations\n",
    "    \n",
    "    def _gradient(self):\n",
    "        # calculate the gradient of w \n",
    "        # your_code\n",
    "        return grad\n",
    "    \n",
    "    def _update(self):\n",
    "        grad = self._gradient()\n",
    "        # update the weights using the gradient and learning rate\n",
    "        self.w =  # your_code\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return logistic(X.dot(self.w))\n",
    "        \n",
    "    def optimize(self):\n",
    "        it = 0\n",
    "        loss = []\n",
    "        # we iterate until we reach self.max_iter\n",
    "        while it < self.max_iter:\n",
    "            # update the weights (use the method you implemented above)\n",
    "            # append the current loss (use self.predict, and the regularizer(), and logloss() functions)\n",
    "            \n",
    "            # your_code\n",
    "            \n",
    "            loss.append(# your_code\n",
    "            it += 1\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the class\n",
    "optimizer = Steepest_descent_optimizer(X_train, y_train, lambd = 0.001, alpha = 0.001)\n",
    "\n",
    "# run the optimization for 10000 steps, this might take a while...\n",
    "loss = optimizer.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot  the evolution of the loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(loss)),np.array(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict from the test set\n",
    "test_pred = optimizer.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(util)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how accurate our predictions are using the average precision score and the roc area under the curve score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output **:  ~ 0.993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output **: ~ 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_confusion_matrix(test_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you made it through one part of the fifth tutorial of this course!\n",
    "\n",
    "# Submitting your assignment\n",
    "\n",
    "Please rename your notebook under your full name and **submit it on the moodle platform**. If you have problems to do so, you can also send it again to machinelearning.dhc@gmail.com\n",
    "\n",
    "Please rename the file to 1_LinRegTut_<GROUP\\>.ipynb and replace <GROUP\\> with your group-name.\n",
    "\n",
    "As this is also the first time for us preparing this tutorial, you are welcome to give us feedback to help us improve this tutorial.  \n",
    "\n",
    "Thank you!  \n",
    "\n",
    "Jana & Remo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
