{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: Neural networks\n",
    "\n",
    "Welcome to the sixth tutorial of the course 'Machine learning for Precision Medicine'.\n",
    "\n",
    "In this tutorial we will implement a neural network architecture, which involves the following steps.\n",
    "\n",
    "1) initialize the weights  \n",
    "2) Forward Proagation  \n",
    "    2.1) Perform linear transformation of input  \n",
    "    2.2) Compute Activations from the linear transformations   \n",
    "3) calculate the loss  \n",
    "4) Backpropagation  \n",
    "5) update weights \n",
    "\n",
    "We will use an artificial dataset here, which we want to separate into two classes. Let's generate the and look at the data first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary python modules\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_circles\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we generate a toy-dataset that is not linearly separable:\n",
    "X, y = make_circles(n_samples=1000, factor=.4, noise=.10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train.ravel(), s=50, cmap=plt.cm.Spectral, edgecolors='black');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can see that the data is not linearly separable. So let's use a neural network model to classify the blue from the red data points. Here we will use a neural network, with 4 hidden layers with 25, 50, 50 and 25 units respectively and an output layer of 2 units for our binary classification (red or blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the lecture, each neuron in our neural network will perform a linear transformation of it's input values, which will produce an intermediate value $z$. We apply a non-linear activation function to $z$ in order to get the *activation* $a$, which in turn will be fed to other neurons, until we arrive at the final neuron(s) which constitute the model output. By sequentially performing many of these operations (linear transformation + non-linear activation), we are able to compute very complex non-linear functions of the input variables.\n",
    "\n",
    "For each neuron, the linear transformation is parameterized by a weight column-vector $\\mathbf{w}$ and a bias parameter $b$. Our non-linear activation functions in this exercise will not have any adjustable parameters.\n",
    "\n",
    "We call a group of neurons which are parameterized in the same way (i.e. their weight vectors have the same length), and perform the same kind of operation on the same input as a *layer*. We can stack the transposed weight vectors of all neurons ${n}$ in the same layer ${l}$ on top of each other to form a weight matrix $\\mathbf{W}^{\\{l\\}}$ and bias vector $\\mathbf{b}^{\\{l\\}}$. Following this definition, the weight matrix of the layer ${l}$, $\\mathbf{W}^{\\{l\\}}$, is an $n^{\\{l\\}}$ (number of neurons in this layer) by $n^{\\{l-1\\}}$ (number of neurons in the previous layer) matrix, and $\\mathbf{b}^{\\{l\\}}$ is a vector of vector of length $n^{\\{l\\}}$.\n",
    "\n",
    "In this exercise, we will look at a certain class of neural networks called a feed-forward or densely connected neural network. In a densely connected neural network, each neuron of a layer is connected to all neurons of the previous layer, i.e. every neuron in layer ${l}$, will recieve all the output $\\mathbf{a}^{\\{l-1\\}}$, where $\\mathbf{a}^{\\{l-1\\}}$ is the vector that results from concatenating the $n^{\\{l-1\\}}$ activations of the previous layer $\\{a_1,a_2,...,a_{n^{\\{l-1\\}}}\\}$. The first layer ${(l = 1)}$, receives the input ${\\mathbf{a}^{\\{0\\}}} = \\mathbf{x}$, where $\\mathbf{x}$ is a single observation in our training set $\\mathbf{X}$.\n",
    "\n",
    "We can express the operations happening within a single layer using matrix multiplaction:\n",
    "$$  \\mathbf{z}^{\\{l\\}} = \\mathbf{W}^{\\{l\\}} \\mathbf{a}^{\\{l-1\\}} + \\mathbf{b}^{\\{l\\}}$$\n",
    "$$  \\mathbf{a}^{\\{l\\}} = \\phi^{\\{l\\}}(\\mathbf{z}^{\\{l\\}})$$\n",
    "\n",
    "where $\\phi^{\\{l\\}}$ is the activation function for layer $l$.\n",
    "\n",
    "Finally, we are not feeding single observation $\\mathbf{x}$ to our network, but rather we are processing an entire batch of observations $\\mathbf{X}_t$, where $\\mathbf{X}_t$ is an $m$ by $i$ matrix, corresponding to $m$ observations $\\mathbf{x}^T$ stacked on top of each other, each having $i$ features. \n",
    "\n",
    "$$  \\mathbf{Z}^{\\{l\\}} = \\mathbf{W}^{\\{l\\}} \\mathbf{A}^{\\{l-1\\}} + \\mathbf{b}^{\\{l\\}} $$\n",
    "$$  \\mathbf{A}^{\\{l\\}} = \\phi^{\\{l\\}}(\\mathbf{Z})  $$\n",
    "\n",
    "Where $\\mathbf{A}^{\\{0\\}} = \\mathbf{X}_t$ and $\\mathbf{b}^{\\{l\\}}$ is added to the matrix $\\mathbf{W}^{\\{l\\}} \\mathbf{A}^{\\{l-1\\}}$ via [broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html), i.e. it is added element-wise to each column. $\\mathbf{A}^{\\{l\\}}$ and $\\mathbf{Z}^{\\{l\\}}$ both have the shape $(n^{\\{l\\}}, m)$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with the implementation, we have to initialize weights across the entire network architechture. How to initialize weights before training is also a big research topic in Deep Learning. Here, we will just use randomly generated numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture, seed = 99):\n",
    "    # random seed initiation\n",
    "    np.random.seed(seed)\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    # parameters storage initiation\n",
    "    params_values = {}\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        # extracting the number of units in layers\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        # initiating the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        # save everything in a dictionary\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to implement the activation functions for our linear transformations. We can activate neurons either with the sigmoid or relu function. \n",
    "\n",
    "In the lecture we were introduced to the ReLU activation function, which will use to activate the neurons in the hidden layers:\n",
    "\n",
    "$$ ReLU(\\mathbf{Z}) = max(0,\\mathbf{Z}) $$\n",
    "\n",
    "Our final output layer will use the sigmoid activation function, which was already introduced in the last exercise:\n",
    "\n",
    "$$ \\sigma(\\mathbf{Z}) = \\frac{1}{1+exp(-\\mathbf{Z})} $$\n",
    "\n",
    "## Task 1:\n",
    "Implement the sigmoid and relu functions, which take the linear transformation Z as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT\n",
    "def sigmoid(Z):\n",
    "    sig = #your_code\n",
    "    return sig\n",
    "\n",
    "def relu(Z):\n",
    "    relu = #your_code\n",
    "    return relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement the forward propagtion of a single layer. This function requires the activations of the previous layer stored in A_prev, the weights stored in W_curr and the bias stored in b_curr, as well as an argument which activation function you want to use. \n",
    "\n",
    "## Task 2: \n",
    "Implement the linear transformation input $\\mathbf{Z}$ of the next layer with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STUDENT\n",
    "\n",
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    # calculation of the input value for the activation function\n",
    "    \n",
    "    Z_curr = #your_code\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    # return of calculated activation A and the intermediate Z matrix\n",
    "    return activation_func(Z_curr), Z_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement the forward propagation through the entire network and call the function above for each layer. \n",
    "The function here requires our input data $\\mathbf{X}$, our initialized weights and biases stored in params_values and the network architecture. The function will output the activation of the last layer, as well as the memory of all activations, weights and biases from the hidden layers below. \n",
    "\n",
    "## Task 3:\n",
    "Call the forward propagation of a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT\n",
    "\n",
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    # creating a temporary memory to store the information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0â€Š\n",
    "    A_curr = X\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        # extraction of W for the current layer\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        # extraction of b for the current layer\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        # calculation of activation for the current layer\n",
    "        \n",
    "        A_curr, Z_curr = #your_code\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # return of prediction vector and a dictionary containing intermediate values\n",
    "    return A_curr, memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compare our output from the output layer $\\hat{\\mathbf{Y}}$ with the true $\\mathbf{Y}$ and calculate the loss, or cost. This is our objective function that we seek to minimize. \n",
    "\n",
    "\\begin{equation}\n",
    "J(w,b) =  -\\frac{1}{m} \\sum_{i=1}^{m}{y log\\hat{y}^{(i)} + (1-y^{(i)}) log(1-\\hat{y}^{(i)})}\n",
    "\\end{equation}\n",
    "\n",
    "## Task 4:\n",
    "Implement the cost, based on the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    # calculation of the cost according to the formula\n",
    "    cost = #your_code\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output layer outputs $\\hat{Y}$ with values between 0 and 1 because it applies the sigmoid function. These values correspond to the probability of belonging to class 1. We now have to set a threshold, which defines, that we assign class 1 to a sample that has a value higher than 0.5 and class 0 if smaller or equals 0.5. Afterwards we calculate the accuracy of our predicted labels, by checking how many $\\hat{\\mathbf{Y}}$ were equals to the true $\\mathbf{Y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "\n",
    "We have now implemented the full forward propagation through the network. We now wish to implement back propagation in a similar way. This will require us to calculate the partial derivatives of our Loss function with respect to the trainable model parameters.\n",
    "\n",
    "For a single layer of our neural network, the gradients are calculated according to the following formulae:\n",
    "\n",
    "$$ \\mathbf{dW}^{\\{l\\}} = \\frac{\\delta L}{\\delta\\mathbf{W}^{\\{l\\}}} = \\frac{1}{m} \\mathbf{dZ}^{\\{l\\}} \\mathbf{A}^{\\{l-1\\}T} $$\n",
    "\n",
    "$$ \\mathbf{db}^{\\{l\\}} = \\frac{\\delta L}{\\delta\\mathbf{b}^{\\{l\\}}} = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbf{dZ}^{\\{l\\}(i)} $$\n",
    "\n",
    "$$ \\mathbf{dA}^{\\{l-1\\}} = \\frac{\\delta L}{\\delta\\mathbf{A}^{\\{l-1\\}}} = \\mathbf{W}^{\\{l\\}T} \\mathbf{dZ}^{\\{l\\}} $$\n",
    "\n",
    "$$ \\mathbf{dZ}^{\\{l\\}} = \\mathbf{dA}^{\\{l\\}} * \\phi^{\\{l\\}'}(\\mathbf{Z}^{\\{l\\}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already saw that these formulae make use of the cached values for $\\{\\mathbf{Z}^{\\{1\\}}, \\mathbf{Z}^{\\{2\\}}, ..., \\mathbf{Z}^{\\{n\\}}\\}$ and $\\{\\mathbf{A}^{\\{1\\}}, \\mathbf{A}^{\\{2\\}}, ..., \\mathbf{A}^{\\{n\\}}\\}$ calculated during forward propagation. In a first step, let's implement the formula for $\\mathbf{dZ}$ for the sigmoid and ReLU activation functions.\n",
    "\n",
    "## Task 5:\n",
    "\n",
    "Implement `relu_backward(dA, Z)` and `sigmoid_backward(dA, Z)`. Both functions take a matrix `dA` ($\\mathbf{dA}$), *which will be passed during back-propagation* and cached values `Z` ($\\mathbf{Z}$), and return $\\mathbf{dA} * \\phi^{'}(\\mathbf{Z})$, where we substitute $\\phi^{'}$ with $ReLU^{'}(z)$ or $\\sigma^{'}(z)$, performed element-wise for all values in $\\mathbf{Z}$, respectively. **Be aware that $*$ here denotes element-wise multiplication, not matrix-multiplication!**\n",
    "\n",
    "$$ \\mathbf{dZ} = \\mathbf{dA} * \\phi^{'}(\\mathbf{Z}) $$\n",
    "\n",
    "$$  ReLU^{'}(z) =   \\begin{equation}\n",
    "   \\begin{cases}\n",
    "     1, & \\text{if}\\ z>0 \\\\\n",
    "     0, & \\text{otherwise}\n",
    "   \\end{cases}\n",
    "\\end{equation} $$\n",
    "\n",
    "$$ \\sigma^{'}(z) = \\sigma (z)\\cdot (1-\\sigma(z)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = #your_code\n",
    "    \n",
    "    return dZ;\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    # tip: make use of the \"sigmoid\"-function we implemented above \n",
    "    sig = #your_code\n",
    "    dZ = #your_code\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6:\n",
    "\n",
    "We now wish to implement a function `single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\")`, where:\n",
    "\n",
    "* `dA_curr` corresponds to $\\mathbf{dA}^{\\{l\\}}$, passed during back propagation, needed to calculate $\\mathbf{dZ}^{\\{l\\}}$\n",
    "* `W_curr` corresponds to $\\mathbf{W}^{\\{l\\}}$, the current weight matrix\n",
    "* `b_curr` corresponds to $\\mathbf{b}^{\\{l\\}}$, the current bias vector \n",
    "* `A_prev` corresponds to $\\mathbf{A}^{\\{l-l\\}}$, cached activation-values of the previous layer, needed to calculate $\\mathbf{dW}^{\\{l\\}}$\n",
    "\n",
    "`single_layer_backward_propagation` should calculate the gradients of the trainable parameters ($\\mathbf{dW}^{\\{l\\}}, \\mathbf{db}^{\\{l\\}}$) for a single layer $l$. It will also calculate $\\mathbf{dZ}^{\\{l\\}}$ (depending on which activation function was used) in order to calculate $\\mathbf{dA}^{\\{l-1\\}}$, which will be passed on to the preceding layer ${l-1}$ during back propagation. Use the formulae introduced above to perform the necessary calculations.\n",
    "\n",
    "It returns `dA_prev`, `dW_curr`, `db_curr`, which correspond to $\\mathbf{dA}^{\\{l-1\\}}$, $\\mathbf{dW}^{\\{l\\}}, \\mathbf{db}^{\\{l\\}}$\n",
    "\n",
    "**IMPORTANT:** when calculating `db_curr`, make use of the function `np.sum(..., axis=..., keepdims=True)`, make sure you set `keepdims=True`, this will ensure that `db_curr` and `b_curr` keep the same dimensions, which is important in oder to perform parameter updates later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT\n",
    "\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    # calculation of the activation function derivative\n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    \n",
    "    # derivative of the matrix W\n",
    "    dW_curr = #your_code\n",
    "    # derivative of the vector b\n",
    "    db_curr = #your_code\n",
    "    # derivative of the matrix A_prev\n",
    "    dA_prev = #your_code\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function `full_forward_propagation` you implemented above, you initialized the activations that feed in to the first layer with ${A^{\\{0\\}}} = X $, i.e. `A_curr = X`. We will also need values $\\mathbf{dA}^{\\{n\\}}$ for or last layer $n$ in order to initialize back-propagation:\n",
    "\n",
    "$$ \\mathbf{dA}^{\\{n\\}} = \\frac{\\delta L}{\\delta\\mathbf{A}^{\\{n\\}}} = -(\\frac{\\mathbf{Y}}{\\mathbf{\\hat{Y}}} - \\frac{1-\\mathbf{Y}}{1-\\mathbf{\\hat{Y}}}) $$\n",
    "\n",
    "where ${\\hat{\\mathbf{Y}}} = \\mathbf{A}^{\\{n\\}}$ are our predicted values for the target variable. We have implemented this calculation for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_backward(Y, Y_hat):\n",
    "    return - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7:\n",
    "\n",
    "Above we implemented the function `full_forward_propagation`, which sequentially iterates over the layers starting from the input layer in order to perfrom forward propagation. It calculates the transformations defined by the weight and bias parameters and activation functions, and stores the intermediate outputs in memory.\n",
    "\n",
    "We now write a similar function called `full_back_propagation`, which iterates over the layers in reverse order, starting from the output layer in order to perform back propagation. It makes use of the intermediate outputs in order to calculate the gradients of the loss function with respect to the model parameters. It stores these gradients in a dictionary `grads_values`, which is returned in the end.\n",
    "\n",
    "Complete the function `full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture)`, where:\n",
    "\n",
    "* `Y_hat` corresponds to $\\hat{\\mathbf{Y}} = \\mathbf{A}^{\\{n\\}}$\n",
    "* `Y` corresponds to $\\mathbf{Y}$, the training data labels, which are eihter 0 or 1\n",
    "* `memory` is the dictionary of cached values for $\\{\\mathbf{Z}^{\\{1\\}}, \\mathbf{Z}^{\\{2\\}}, ..., \\mathbf{Z}^{\\{n\\}}\\}$ and $\\{\\mathbf{A}^{\\{1\\}}, \\mathbf{A}^{\\{2\\}}, ..., \\mathbf{A}^{\\{n\\}}\\}$\n",
    "* `params_values` is the dictionary of current parameter values, i.e. $\\{\\mathbf{W}^{\\{1\\}}, \\mathbf{W}^{\\{2\\}}, ..., \\mathbf{W}^{\\{n\\}}\\}$ and $\\{\\mathbf{b}^{\\{1\\}}, \\mathbf{b}^{\\{2\\}}, ..., \\mathbf{b}^{\\{n\\}}\\}$\n",
    "* `nn_architecture` is the dictionary that defines the model architecture\n",
    "\n",
    "Here, you only have to call the single_layer_backward_propagation() function with the right parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    m = Y.shape[1]\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # initiation of gradient descent algorithm\n",
    "    dA_prev = loss_backward(Y, Y_hat)\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = #your_code\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to get the gradients for all the trainable parameters, it is time to write a function that will allow us to update the parameters using the update rule introduced in the last exercise:\n",
    "\n",
    "$$ \\mathbf{W}^{\\{l\\}} = \\mathbf{W}^{\\{l\\}} - \\alpha \\mathbf{dW}^{\\{l\\}} $$\n",
    "$$ \\mathbf{b}^{\\{l\\}} = \\mathbf{b}^{\\{l\\}} - \\alpha \\mathbf{db}^{\\{l\\}} $$\n",
    "\n",
    "## Task 8:\n",
    "\n",
    "Complete the function `update` below. It takes the following parameters:\n",
    "\n",
    "* `params_values` dictionary of parameter values $\\{\\mathbf{W}^{\\{1\\}}, \\mathbf{W}^{\\{2\\}}, ..., \\mathbf{W}^{\\{n\\}}\\}$ and $\\{\\mathbf{b}^{\\{1\\}}, \\mathbf{b}^{\\{2\\}}, ..., \\mathbf{b}^{\\{n\\}}\\}$\n",
    "* `grads_values` dictionary of gradients for the trainable parameters $\\{\\mathbf{dW}^{\\{1\\}}, \\mathbf{dW}^{\\{2\\}}, ..., \\mathbf{dW}^{\\{n\\}}\\}$ and $\\{\\mathbf{db}^{\\{1\\}}, \\mathbf{db}^{\\{2\\}}, ..., \\mathbf{db}^{\\{n\\}}\\}$\n",
    "* `nn_architecture` dictionary defining the neural network architecture\n",
    "* `learning_rate`, the learning rate $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT\n",
    "\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    # iteration over network layers\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values[\"W\" + str(layer_idx)] = #your_code        \n",
    "        params_values[\"b\" + str(layer_idx)] = #your_code \n",
    "\n",
    "    return params_values;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9:\n",
    "Now we have everything we need to train our model. The final task of this exercise, is to insert the functions you implemented above in the right places below. If you understood what you are doing, this should be more or less self-explanatory ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT\n",
    "\n",
    "def train(X, Y, nn_architecture, epochs, learning_rate, verbose=False):\n",
    "    # initiation of neural net parameters\n",
    "    \n",
    "    params_values = #your_code\n",
    "    \n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    # performing calculations for subsequent iterations\n",
    "    for i in range(epochs):\n",
    "        # step forward\n",
    "        Y_hat, cache = #your_code\n",
    "        \n",
    "        # calculating metrics and saving them in history\n",
    "        cost = get_cost_value(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        accuracy = get_accuracy_value(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        # step backward - calculating gradient\n",
    "        grads_values = #your_code\n",
    "        \n",
    "        # updating model state\n",
    "        params_values = #your_code\n",
    "        \n",
    "        if(i % 50 == 0):\n",
    "            if(verbose):\n",
    "                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n",
    "            \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "params_values, cost_history, accuracy_history = train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), NN_ARCHITECTURE, 10000, 0.01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(X_test), params_values, NN_ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy achieved on the test set\n",
    "acc_test = get_accuracy_value(Y_test_hat, np.transpose(y_test.reshape((y_test.shape[0], 1))))\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last but not least, let's plot how the accuracy and cost evolved over the training epochs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(10000), np.array(cost_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(10000), np.array(accuracy_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "What can you say about the learning progress of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "Can you find out how many trainable parameters our model contains? Do you think that this number of parameters is appropriate for our classification task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you made it through the sixth tutorial of this course!\n",
    "\n",
    "# Submitting your assignment\n",
    "\n",
    "Please rename your notebook under your full name and **submit it on the moodle platform**. If you have problems to do so, you can also send it again to machinelearning.dhc@gmail.com\n",
    "\n",
    "Please rename the file to 1_LinRegTut_<GROUP\\>.ipynb and replace <GROUP\\> with your group-name.\n",
    "\n",
    "As this is also the first time for us preparing this tutorial, you are welcome to give us feedback to help us improve this tutorial.  \n",
    "\n",
    "Thank you!  \n",
    "\n",
    "Jana & Remo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
